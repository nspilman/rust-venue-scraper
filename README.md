# SMS Server Rust - Event Data Scraper

A modular, asynchronous Rust-based event data scraper with database persistence, clean architecture, and extensible API support. Part of the Seattle Music Scene (SMS) project.

## ğŸ“¦ Features

- **ğŸ¢ Database Persistence**: Full Turso/libSQL integration with graph-based schema
- **ğŸ’¬ In-Memory Mode**: Fast development and testing without persistence  
- **ğŸ”„ ETL Pipeline**: Complete Extract, Transform, Load architecture
- **ğŸ”¨ Smart Processing**: Venue/artist matching, deduplication, and relationship tracking
- **ğŸš€ Async Architecture**: High-performance concurrent processing
- **ğŸ“Š Comprehensive Metrics**: Phase-based Prometheus metrics for complete pipeline observability

## Architecture

The scraper exhibits a modular and extensible architecture with clean separation of concerns:

### Core Components

- **EventApi Trait**: Central abstraction that all crawler modules implement, defining the contract for API-specific logic
- **Pipeline Module**: Handles raw event ingestion, processing each raw event into structured `ProcessedEvent` data
- Processing: Handles stored raw data creating/updating domain objects (venues, artists, events) with change tracking
- **Storage Abstraction**: `Storage` trait abstracting all persistence operations with in-memory implementation for development

### Features

- **Modular Design**: Independent development and extension of new crawlers without impacting core pipeline logic
- **Asynchronous Processing**: Concurrent fetching and processing using `tokio` runtime for improved scalability
- **Comprehensive Logging**: Detailed observability via `tracing` with structured error handling using `anyhow` and `thiserror`
- **Flexible Execution**: CLI supports running ingestion, processing, or both sequentially on specified APIs

## Current API Implementations

- **Blue Moon**: Venue-specific event scraping
- **Sea Monster**: Lounge event data extraction

## ğŸš€ Usage

### Quick Start - API Endpoints

Once the services are running, you can access:

**GraphQL API** (port 8080):
- GraphQL Playground: http://localhost:8080/graphql
- Raw GraphQL endpoint: `curl -X POST http://localhost:8080/graphql -H "Content-Type: application/json" -d '{"query":"{ events { id title venue { name } artists { name } } }"}'`

**Web Interface** (port 3001):
- Events listing: http://localhost:3001/events
- Search events: http://localhost:3001/events?search=jazz
- Filter by venue: http://localhost:3001/events?venue=neumos
- View artist: http://localhost:3001/artist/[artist-id]
- View venue: http://localhost:3001/venue/[venue-id]

**Health & Metrics**:
- API health check: http://localhost:8080/health
- Prometheus metrics: http://localhost:9464/metrics
- Grafana dashboard: http://localhost:3000 (admin/admin)

### Database Setup (Optional)

For persistent storage, set up a Turso database:

```bash
# Copy environment template
cp .env.example .env

# Edit .env with your Turso credentials:
# LIBSQL_URL=libsql://your-database.turso.io
# LIBSQL_AUTH_TOKEN=your_auth_token
```

### Running the Scraper

```bash
# Build the project
cargo build

# Run ingester only (in-memory)
cargo run -- ingester --apis blue_moon,sea_monster

# Run ingester with database persistence
cargo run -- ingester --apis blue_moon --use-database


# Test database connection
cargo run --bin test_db

# Run integration tests
cargo run --bin test_integration

# Run with metrics (includes server with /metrics endpoint)
cargo run -- server --port 8080
```

### ğŸ“Š Metrics & Observability

The scraper includes comprehensive Prometheus-compatible metrics:

```bash
# Run complete pipeline demonstration with metrics
./demo-local.sh

# View metrics in real-time
curl http://localhost:9898/metrics

# Start with Docker Compose (includes Prometheus + Grafana)
docker-compose up -d
```

Metrics cover all pipeline phases:
- **Sources**: Request success/failure, durations, payload sizes
- **Gateway**: Envelope processing, deduplication rates, CAS operations  
- **Parser**: Parsing performance, record production, error rates
- **Ingest Log**: Write operations, consumer lag, file rotations

See [METRICS.md](METRICS.md) for complete documentation.

## Configuration

Configuration is managed via `config.toml` with support for:
- API endpoints and credentials
- Rate limiting settings
- Logging levels
- Storage configuration

## ğŸ† Architecture Score: 5.0/5

**Strengths:**
- âœ… Loose coupling and clear abstractions
- âœ… Separation of concerns between ingestion and processing
- âœ… Extensible plugin-like architecture for new APIs
- âœ… Scalable asynchronous design
- âœ… **Full database persistence with graph schema**
- âœ… **Dual storage modes (in-memory + database)**
- âœ… **Complete ETL pipeline with audit logging**

**Future Enhancements:**
- âœ… ~~Persistent storage backend~~ â†’ **Turso/libSQL implemented**
- Enhanced concurrent processing in processing stage
- External API integrations (Ticketmaster, Eventbrite, etc.)
- Advanced artist parsing with NLP
- GraphQL API layer for data access
- Web dashboard for monitoring and management

## Archive

The historical â€œcarpenterâ€ implementation has been preserved for reference in archive/carpenter.rs. The live codebase no longer uses or references it.

## Development

The project uses standard Rust tooling:
- `cargo build` - Build the project
- `cargo test` - Run tests
- `cargo run` - Execute the scraper

Logs are written to `logs/` directory and excluded from version control.
